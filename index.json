
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I am an assistant professor at Computer Science and Engineering Department in Shanghai Jiao Tong University (SJTU). I received the Master and Ph.D. degrees from Shanghai Jiao Tong University under the supervision of Prof. Quan Chen and Prof. Minyi Guo. For now, I still work closely with Prof. Quan Chen and Assist Prof. Weihao Cui.\nMy past research focused on High performance computing on CPU architecture, Task scheduling on GPU architecture, Resource management in Datacenter, and DNN inference system design. My future research will cover more cloud computing topics such as GPU serverless and GPU virtualization. Specically, my future research could be divided into four aspects: Fine-grained GPU resource division, Multi-worker scheduling for DNN tasks, GPU serverless and Resource management in Datacenter.\nI am now looking for perspective Ph.D students and Master Students. If you are interested in above areas, we should talk.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am an assistant professor at Computer Science and Engineering Department in Shanghai Jiao Tong University (SJTU). I received the Master and Ph.D. degrees from Shanghai Jiao Tong University under the supervision of Prof.","tags":null,"title":"Han Zhao 赵涵","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://example.com/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Chuhao Xu","Yiyu Liu","Zijun Li","Quan Chen","Han Zhao","Deze Zeng","Qian Peng","Xueqi Wu","Haifeng Zhao","Senbo Fu","Minyi Guo"],"categories":null,"content":"","date":1709510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1709510400,"objectID":"6cf1f8ec21ac655e16e1d07e71275587","permalink":"https://example.com/publication/2024bless/","publishdate":"2024-03-04T00:00:00Z","relpermalink":"/publication/2024bless/","section":"publication","summary":"In serverless computing, an idle container is not recycled directly, in order to mitigate time-consuming cold container startup. These idle containers still occupy the memory, exasperating the memory shortage of today's data centers. By offloading their cold memory to remote memory pool could potentially resolve this problem. However, existing offloading policies either hurt the Quality of Service (QoS) or are too coarse-grained in serverless computing scenarios. We therefore propose FaaSMem, a dedicated memory offloading mechanism tailored for serverless computing with memory poor architecture. It is proposed based on our finding that the memory of a serverless container allocated in different stages has different usage patterns. Specifically, FaaSMem proposes Page Bucket (Pucket) to segregate the memory pages in different segments, and applies segment-wise offloading policies for them. FaaSMem also proposes a semi-warm period during keep-alive stage, to seek a sweet spot between the offloading effort and the remote access penalty. Experimental results show that FaaSMem reduces the average local memory footprint by 9.9% - 79.8% and improves the container deployment density to 108% - 218%, with negligible 95%-ile latency increase.","tags":[],"title":"Improving the Multi-Tenancy GPU Performance through Adaptive Bubbleless Spatial-Temporal Sharing","type":"publication"},{"authors":["Shulai Zhang","Quan Chen","Weihao Cui","Han Zhao","Chunyu Xue","Zhen Zheng","Wei Lin","Minyi Guo"],"categories":null,"content":"","date":1707004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707004800,"objectID":"18e9cfe1e86f3605986f9b4fd559abb1","permalink":"https://example.com/publication/2024faasmem/","publishdate":"2024-02-04T00:00:00Z","relpermalink":"/publication/2024faasmem/","section":"publication","summary":"While GPUs are becoming more powerful, cloud providers start to allow multiple tenants that often have lightweight workloads to share a GPU. Existing temporal or spatial shar- ing systems struggle to provide efficient and accurate quota assignments for tenants. We observe that the performance of the multi-tenancy system is often underestimated because of the existence of unused GPU “bubbles” and can be enhanced by squeezing the bubbles. Based on this observation, we de- sign Bless, a bubble-less spatial-temporal sharing GPU sys- tem that fine-tunes the GPU resource allocation to improve multi-tenancy performance. Bless leverages precise comput- ing resource management and fine-grained kernel schedul- ing to ensure stringent quota guarantees and reduce latency fairly for tenants with varying GPU quotas. We implement and evaluate Bless with multiple applications and work- loads. Our result shows that Bless achieves 21.1% − 37.3% average latency reduction over the state-of-the-art while guaranteeing the promised quota for all tenants.","tags":[],"title":"Improving the Multi-Tenancy GPU Performance through Adaptive Bubbleless Spatial-Temporal Sharing","type":"publication"},{"authors":null,"categories":null,"content":"Overview 服务器无感知计算是新一代云计算云原生中重要的组成部分。由于服务器无感知计算带来 了基础设施托管，按需付费，细粒度资源使用等优势，其在近几年内取得了迅猛的发展。然而 ，目前的服务器无感知计算系统仅支持使用通用处理器CPU的函数（CPU函数），而对使用加速 器的函数（加速器函数）的支持十分有限。这是因为加速器的独特设计给服务器无感知计算带 来了多个挑战。首先，目前加速器并不支持精确的细粒度资源使用，直接发射加速器函数到加 速器上导致加速器承接函数密度较低，进而带来了加速器的低吞吐。其次，加速器的使用需要 依赖主机端的CPU辅助，额外的执行阶段准备导致函数启动缓慢，进而带来了函数的高延迟。 面对以上挑战，本项目创新性地从软件侧角度出发，考虑多种加速器的通用特征，设计了加速 器细粒度共享方法、加速器函数低延迟调度方法、以及易使用高性能的编程模型，以支持加速 器函数高效共享加速器。\n","date":1704153600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704153600,"objectID":"15d324856eee1ac9a279a89d42abe0ce","permalink":"https://example.com/post/2024-%E9%9D%92%E5%B9%B4%E5%9F%BA%E9%87%91/","publishdate":"2024-01-02T00:00:00Z","relpermalink":"/post/2024-%E9%9D%92%E5%B9%B4%E5%9F%BA%E9%87%91/","section":"post","summary":"青年科学基金项目, 项目负责人, 2024-2027","tags":null,"title":"服务器无感知计算的加速器高效共享研究","type":"post"},{"authors":null,"categories":null,"content":"Overview 高性能计算是世界各国竞相争夺的战略制高点，是国家综合国力和科技创新力的重要指标。“高性能计算”重点专项以研制新一代国产超算系统及应用为目标，聚焦高性能计算机研制、系统与计算应用软件研发、超算互联网构建三个主要方向，统筹考虑并制定高性能计算技术的战略研究路线，持续保持我国高性能计算机研制与应用水平处于世界领先地位。随着摩尔定律发展的逐渐放缓，基于异构处理器的高性能计算机逐渐成为主流。天河、神威、曙光系列等国产超算系统的异构处理器架构复杂多样，显著增大了并行编程和不同国产超算系统间的并行程序移植的难度。因此，研究面向新一代国产超算系统的统一并行编程模型与并行编译，对构建可持续发展的高性能计算生态环境有着重要的支撑作用。\n针对上述问题，本项目拟研制面向国产超算系统的统一并行编程模型、编程语言、并行编译系统和运行调度框架，支持并行程序在国内主流超算系统上的程序移植，包括神威系列、天河系列、曙光系列等。采用该并行编程模型编写的并行程序，无需修改代码就能够直接在国内自主研制的多台国产超算系统上直接编译与运行，执行性能达到本地编译系统性能的80%以上，为解决不同国产超算系统间的并行程序移植难题提供保障。\n本项目针对如何在不同国产超算系统上实现高效率的统一编程、如何在多样化架构上构建统一编译中间表示、如何在复杂多样的国产异构处理器上生成高效目标代码、如何在超大规模异构高性能计算机上提供高效运行时支持四个关键科学问题开展研究。本项目涉及的研究任务划分为四个研究课题：（1）统一并行编程模型与示范应用：研究支持描述不同异构处理器硬件特性的统一编程模型和编程语言，实现不同国产超算系统的统一编程，并基于此研究高层抽象，进一步降低并行编程复杂性，同时通过示范应用开展技术验证；（2）统一中间表示与多层次优化：建立面向不同国产超算系统的统一中间表示，研究基于统一中间表示的多层次优化技术，提炼跨国产超算系统的共性编译优化，构建适用于多台国产高性能计算机的共性编译基础设施；（3）体系结构感知的并行编译优化：研究感知国产超算体系结构的并行编译优化方法，全方位感知加速单元、多级存储层次、微架构等硬件特性，同时研究针对不同架构的自调优技术，最大程度地释放国产超算的计算潜能。（4）异构超大规模系统运行时设计与优化：研究面向统一并行编程模型的公共运行时实现，并研究感知不同异构硬件拓扑的特色运行时优化，同时研究弹性调度技术支持并行程序扩展至超大规模系统。\n","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"bc01842df818056a6b71d7e31dc240f4","permalink":"https://example.com/post/2024-%E9%87%8D%E7%82%B9%E7%A0%94%E5%8F%912/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/post/2024-%E9%87%8D%E7%82%B9%E7%A0%94%E5%8F%912/","section":"post","summary":"国家重点研发计划, 子课题负责人, 2024-2027","tags":null,"title":"面向新一代国产超算系统的统一并行编程模型与并行编译","type":"post"},{"authors":["Binghao Chen","Han Zhao*","Weihao Cui","Yifu He","Shulai Zhang","Quan Chen","Zijun Li","Minyi Guo"],"categories":null,"content":"","date":1693612800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693612800,"objectID":"14e98a98936e5da2501549c5de2ddf2e","permalink":"https://example.com/publication/2023combo/","publishdate":"2023-09-02T00:00:00Z","relpermalink":"/publication/2023combo/","section":"publication","summary":"Cloud vendors are now providing cloud gaming services with GPUs. GPUs in cloud gaming experience periods of idle because not every frame in a game always keeps the GPU busy for rendering. Previous works temporally co-locate games with best-effort applications to harvest these idle cycles. However, these works ignore the spatial sharing of GPUs, leading to not maximized throughput improvement. The newly introduced RT (ray tracing) Cores inside GPU SMs for ray tracing exacerbate the situation. This paper presents Combo, which efficiently leverages two-level spatial sharing, intra-SM and inter-SM sharing, for throughput improvement while guaranteeing the QoS of rendering games’ frames. Combo is novel in two ways. First, based on the investigation of programming models for RT Cores, Combo devises a neat compilation method to convert the kernels that use RT Cores for fine-grained resource management. We utilize the fine-grained kernel management to construct spatial sharing schemes. Second, since the performance of spatial sharing varies with the actual co-located kernels, two efficient spatial sharing schemes are proposed, exact integrated SM sharing and relaxed intra-SM sharing. In order to maximize the throughput of BE applications, Combo identifies the best-fit scenarios for these two schemes by considering runtime rendering load. Our evaluation shows Combo can achieve up to 38.2% (14.0% on average) throughput improvement compared with the state-of-the-art temporal-only solution.","tags":[],"title":"Maximizing the Utilization of GPUs Used by Cloud Gaming through Adaptive Co-location with Combo","type":"publication"},{"authors":["Han Zhao","Weihao Cui","Quan Chen","Jingwen Leng","Deze Zeng","Minyi Guo"],"categories":null,"content":"","date":1691107200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691107200,"objectID":"3aebd4ff71991c19d5a3c049da96c3ae","permalink":"https://example.com/publication/2023soda/","publishdate":"2023-08-04T00:00:00Z","relpermalink":"/publication/2023soda/","section":"publication","summary":"While deep neural network (DNN) models are mainly trained using GPUs, many companies and research institutions build shared GPU clusters. These clusters host DNN training jobs, DNN inference jobs, and CPU jobs (jobs in traditional areas). DNN training jobs require GPU for main computation and CPU for auxiliary computation. Some DNN inference jobs could rely solely on CPU, while others must utilize both CPU and GPU. Our investigation demonstrates that the number of cores allocated to a training job signiﬁcantly impacts its performance, and that DNN inference jobs can make use of the limited CPU cores on the GPU nodes. To accomplish this, we characterize representative deep learning models in terms of their CPU core requirements for their training jobs and inference jobs, and investigate their sensitivity to other CPU-side resource contention. Based on the characterization, we propose SODA, a scheduling system comprised of an adaptive CPU allocator, a multi-array job scheduler, a hardware-aware inference job placer, and a real-time contention eliminator. The experimental results indicate that SODA increases GPU utilization by an average of 19.9%, while maintaining the quality of service target for all DNN inference jobs and the queuing performance of CPU jobs.","tags":[],"title":"Improving Cluster Utilization Through Adaptive Resource Management for Deep Neural Network and CPU Jobs Colocation","type":"publication"},{"authors":null,"categories":null,"content":"Overview 计算技术正由通用计算转变为领域专用计算，并迈向大数据、图计算与人工智能等相互结合的多领域融合计算新时代。多领域融合计算存在数据-控制-访存密集的时空交织复合特征，给单一数据流或者控制流架构的现有计算机系统带来了数据频繁交互导致性能低、编程效率与可用性差、资源分配固化利用率低下等系列挑战。研究控制流与数据流结合的处理器架构，并打造软硬一体的数据流系统，是构建面向新兴多领域融合计算负载的高性能、高效能、高易用的计算系统的关键。\n本项目紧密围绕重点专项的总体目标，面向国家战略需求、经济与社会重大挑战，在充分考虑现有新型计算负载中存在的数据-控制-访存密集的时空交织复合特征给现有单一数据流或控制流计算系统的难题，解决“数据流与控制流异构核心高效融合、异构融合架构下的编译优化框架、动态运行时系统、真实可用系统”4个科学问题，研制数据流异构融合处理器及数据流软硬一体通用计算系统，构建通用数据流计算技术体系，在关键技术上申请10项专利，形成专利群，发表高水平学术论文20篇，在天文大数据与科学计算、金融图分析、云计算等多个领域得到应用并进行验证。系统关键技术指标包括：研制数据流核心与控制流核心异构融合架构及数据流异构处理器芯片，核数不少于4核，核种类不少于2种，主频1GHz以上，访存带宽150Gbps以上；在相同核数下，峰值性能较经典控制流CPU处理器提升10倍以上，峰值能效提高5倍以上；研制高性能编译工具链，支持跨架构的并行优化，性能提升1倍以上；研制动态支撑运行系统，支持3种以上的典型场景下的通用加速能力开发套件；研制多个交叉领域的高性能算子库及不少于3套加速能力开发套件，支持核心算子100个以上。\n充分发挥数据流技术所具有的数据驱动执行、细粒度异步并行效率高等优势，以其为核心贯穿软硬件边界，研制以数据流为抽象的统一化上层编程模型，以及底层可支撑低开销细粒度并行的控制流-数据流异构融合架构。开展异构数据流处理器核及SoC设计验证，并基于先进工艺完成流片，研制异构数据流实用计算系统，研发异构并行的高性能代码自动生成编译技术以及保证服务质量的动态运行时系统。\n","date":1677628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677628800,"objectID":"f3a8f3b8317cc01edc2d07113cf257f8","permalink":"https://example.com/post/2023-%E9%87%8D%E7%82%B9%E7%A0%94%E5%8F%911/","publishdate":"2023-03-01T00:00:00Z","relpermalink":"/post/2023-%E9%87%8D%E7%82%B9%E7%A0%94%E5%8F%911/","section":"post","summary":"国家重点研发计划, 子课题负责人, 2023-2025","tags":null,"title":"新型数据流异构处理器架构及计算系统","type":"post"},{"authors":["Han Zhao","Weihao Cui","Quan Chen","Minyi Guo"],"categories":null,"content":"","date":1664755200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664755200,"objectID":"2cd5788e96f147b85fa08d85895dc6c2","permalink":"https://example.com/publication/2022ispa/","publishdate":"2022-10-03T00:00:00Z","relpermalink":"/publication/2022ispa/","section":"publication","summary":"Emerging GPUs have multiple Streaming Multiprocessors (SM), while each SM is comprised of CUDA Cores and Tensor Cores. While CUDA Cores do the general computation, Tensor Cores are designed to speed up matrix multiplication for deep learning applications. However, a GPU kernel often either uses CUDA Cores or Tensor Cores, leaving the other processing units idle. Although many prior research works have been proposed to co-locate kernels to improve GPU utilization, they cannot leverage the Intra-SM CUDA Core-Tensor Core Parallelism. Speciﬁcally, ISPA designs persistent and elastic block to solve the thread slot and shared memory contention between co-located kernels. ISPA also adopts the register allocation method to manage the register contention. These resource management methods are applicable for both white-box kernels and cudnn kernels. Experimental results on an Nvidia 2080Ti GPU show that ISPA improves the system-wide throughput by 15.3% for white-box workloads, and 7.1% for cudnn-based workloads compared with prior co-location work.","tags":[],"title":"ISPA: Exploiting Intra-SM Parallelism in GPUs via Fine-grained Resource Management","type":"publication"},{"authors":null,"categories":null,"content":"Overview 对数据中心常用的众多应用而言，并行化是性能提升的主要手段，现在最常用的并行框架是OpenMP和MPI。但是很多应用在编写时并未考虑到使用并行框架，存在着很大的性能提升空间。当前多机并行编程一般采用MPI，但MPI需要编程人员显式分割数据和排布通信。此外MPI应用的部署与执行环境也有强绑定关系。因此，MPI应用的开发是相对困难的。本课题意在基于现有单节点并行编程接口（OpenMP），通过导语或者模板扩展等低侵入方式，设计一套新编程方法或者接口，并实现编译时自动代码生成，生成代码能够在多节点集群场景下，实现高效并行运行。\n具体项目目标如下：\n(1)\t通过识别OpenMP应用程序关键导语，将软件代码自动转换为MPI多节点程序运行：\n将OpenMP单节点应用，自动翻译为MPI多节点应用程序代码； 可以适当增加新的OpenMP导语，实现MPI程序的有效转换； 可以识别串行for循环代码段，自动翻译为并行计算的应用程序； 新增OpenMP导语言，对用户开发习惯影响小。 (2)\t自动进行数据划分，可以均衡地有效地利用多个节点资源进行计算：\n业界通用数据结构：针对规则数据（如普通矩阵）或不规则数据（如稀疏矩阵）可以完成均分，各节点之间计算负载波动不超过5%； 自定义数据结构：针对规则数据（如普通矩阵），尤其是不规则数据（如稀疏矩阵）可以完成均分，各节点之间计算负载波动不超过5%； 多节点计算结果：整形计算结果正确，基于模型，单精度计算结果误差不超过10e6，双精度不超过10e-13。 (3)\t运行时库可以根据鲲鹏架构特点结合数据分割方法实现软件性能最优：\nOpenMP自动翻译多节点程序性能达到手动转换性能70%~80%以上； 通过NPB，lammps，以及minivite、武汉超算客户HPC应用等验证。 ","date":1661990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661990400,"objectID":"8da2bfa1daa1d001b90c2199925095b5","permalink":"https://example.com/post/2022-huawei-1/","publishdate":"2022-09-01T00:00:00Z","relpermalink":"/post/2022-huawei-1/","section":"post","summary":"华为委托研究项目, 项目负责人, 2022-2023","tags":null,"title":"单节点到多节点并行应用源到源翻译工具项目","type":"post"},{"authors":null,"categories":null,"content":"Overview 对数据中心常用的众多应用而言，并行化是性能提升的主要手段，现在最常用的并行框架是openMP和MPI。但是很多应用在编写时并未考虑到使用并行框架，存在着很大的性能提升空间。本项目意在设计一个开发工具，通过分析用户的源码，提取出潜在并行区，并分析并行的可行性，向用户提供并行优化的建议，提高应用的性能表现。\n具体项目目标如下：\n(1)\t检查用户的C/C++/Fortran 应用源码中的并行区源码，识别关键数据及数据依赖关系：\n用户源码中已经使用openMP线程并行的，使用MPI原语进行多节点并行扩展，并通过适当的通信原语保证应用的正确性； 用户源码中已经使用MPI原语进行多节点并行扩展的，发现MPI并行区中可并行部分，并通过openMP导语进行多线程扩展。 (2)\t构建鲲鹏架构线程、进程并行模型，评估对源程序进行线程并行改造收益、进行进程并行改造收益，并根据评估结论向用户提供源码修改建议，包括但不限于：\n使用openMP导语修改应用，并提供关键依赖数据的修改建议； 使用MPI原语修改应用，并提供MPI关键数据分片、通信原语修改建议。 ","date":1661990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661990400,"objectID":"6b32b5ebf525d3734518eaa90a4d5663","permalink":"https://example.com/post/2022-huawei-2/","publishdate":"2022-09-01T00:00:00Z","relpermalink":"/post/2022-huawei-2/","section":"post","summary":"华为委托研究项目, 项目负责人, 2022-2023","tags":null,"title":"源码并行化检测与提示工具项目","type":"post"},{"authors":null,"categories":null,"content":"Overview 国防科技创新特区项目。\n","date":1659312000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659312000,"objectID":"52513300be97e53445eb7db61785c12b","permalink":"https://example.com/post/2022-%E5%9B%BD%E9%98%B2%E4%BA%91/","publishdate":"2022-08-01T00:00:00Z","relpermalink":"/post/2022-%E5%9B%BD%E9%98%B2%E4%BA%91/","section":"post","summary":"国防科技创新特区项目, Main Student Member, 2020-2022","tags":null,"title":"云端协同海洋目标识别与分析","type":"post"},{"authors":["Weihao Cui","Han Zhao","Quan Chen","Hao Wei","Zirui Li","Deze Zeng","Chao Li","Minyi Guo"],"categories":null,"content":"","date":1649721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649721600,"objectID":"3b514d64d1f04bef08c99c97cbe18c03","permalink":"https://example.com/publication/2022dvabatch/","publishdate":"2022-04-12T00:00:00Z","relpermalink":"/publication/2022dvabatch/","section":"publication","summary":"The DNN inferences are often batched for better utilizing the hardware in existing DNN serving systems. However, DNN serving exhibits diversity in many aspects, such as input, operator, and load. The unawareness of these diversities results in inefficient processing. Our investigation shows that the inefficiency roots in the feature of the existing batching mechanism- one entry and one exit. Therefore, we propose DVABatch, a runtime batching system that enables the multi-entry multiexit batching scheme. We first abstract three meta operations, new, stretch, and split, for adjusting the ongoing batch of queries to achieve the multi-entry multi-exit scheme. The meta operations could be used to form different scheduling logics for different diversities. To deliver the meta operations to an ongoing batch, we slice the DNN models into multiple stages. Each stage corresponds to one executor, which is managed by a state transition diagram. Compared with state-of-the-art solutions, our experimental results show that DVABatch reduces 46.4% average latency and achieves up to 2.12× throughput improvement.","tags":[],"title":"DVABatch: Diversity-aware Multi-Entry Multi-Exit Batching for Efficient Processing of DNN Services on GPUs","type":"publication"},{"authors":["Han Zhao","Weihao Cui","Quan Chen","Youtao Zhang","Yanchao Lu","Chao Li","Jingwen Leng","Minyi Guo"],"categories":null,"content":"","date":1649376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649376000,"objectID":"8faef015b3092521153503c51ca6410c","permalink":"https://example.com/publication/2022tacker/","publishdate":"2022-04-08T00:00:00Z","relpermalink":"/publication/2022tacker/","section":"publication","summary":"The proliferation of machine learning applications has promoted both CUDA Cores and Tensor Cores’ integration to meet their acceleration demands. While studies have shown that co-locating multiple tasks on the same GPU can effectively improve system throughput and resource utilization, existing schemes focus on scheduling the resources of traditional CUDA Cores and thus lack the ability to exploit the parallelism between Tensor Cores and CUDA Cores. In this paper, we propose Tacker, a static kernel fusion and scheduling approach to improve GPU utilization of both types of cores while ensuring the QoS (Quality-of-Service) of co-located tasks. Tacker consists of a Tensor-CUDA Core kernel fuser, a duration predictor for fused kernels, and a runtime QoS-aware kernel manager. The kernel fuser enables the ﬂexible fusion of kernels that use Tensor Cores and CUDA Cores, respectively. The duration predictor precisely predicts the duration of the fused kernels. Finally, the kernel manager invokes the fused kernel or the original kernel based on the QoS headroom of latency-critical tasks to improve the system throughput. Our experimental results show that Tacker improves the throughput of best-effort applications compared with state-of-the-art solutions by 18.6% on average, while ensuring the QoS of latency-critical tasks.","tags":[],"title":"Tacker:Tensor-CUDA Core Kernel Fusion for Improving the GPU Utilization while Ensuring QoS","type":"publication"},{"authors":["Weihao Cui","Han Zhao","Quan Chen","Ningxin Zheng","Jingwen Leng","Jieru Zhao","Zhuo Song","Tao Ma","Yong Yang","Chao Li","Minyi Guo"],"categories":null,"content":"","date":1635638400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635638400,"objectID":"ae77344fd7ff0c1560b0cdba81e9a2c8","permalink":"https://example.com/publication/2021abacus/","publishdate":"2021-10-31T00:00:00Z","relpermalink":"/publication/2021abacus/","section":"publication","summary":"While user-facing services experience diurnal load patterns, colocating services improve hardware utilization. Prior work on colocating services on GPUs run queries sequentially, as the latencies of the queries are neither stable nor predictable when running simultaneously. The input sensitiveness and the non-deterministic operator overlap are two primary factors of the latency unpredictability. Hence, We propose Abacus, a runtime system that runs multiple services simultaneously. Abacus enables deterministic operator overlap to enforce latency predictability. Abacus composes of an overlap-aware latency predictor, a headroom-based query controller, and segmental model executors. The predictor predicts the latencies of the deterministic operator overlap. The controller determines the appropriate operator overlap for the QoS guarantee of all the services. The executors run the operators as needed to support the deterministic operator overlap. Our evaluation shows that Abacus reduces 51.3% of the QoS violation and improves the throughput by 29.8% on average compared with state-of-the-art solutions.","tags":[],"title":"Enable Simultaneous DNN Services Based on Deterministic Operator Overlap and Precise Latency Prediction","type":"publication"},{"authors":["Han Zhao","Weihao Cui","Quan Chen","Jieru Zhao","Jingwen Leng","Minyi Guo"],"categories":null,"content":"","date":1629244800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629244800,"objectID":"70099e905ed2c47e1281d3e73a821a79","permalink":"https://example.com/publication/2021plasticine/","publishdate":"2021-08-18T00:00:00Z","relpermalink":"/publication/2021plasticine/","section":"publication","summary":"Emerging GPUs have multiple Streaming Multiprocessors (SM), while each SM is comprised of CUDA Cores and Tensor Cores. While CUDA Cores do the general computation, Tensor Cores are designed to speed up matrix multiplication for deep learning applications. However, a GPU kernel often either uses CUDA Cores or Tensor Cores, leaving the other processing units idle. Although many prior research works have been proposed to co-locate kernels to improve GPU utilization, they cannot leverage the Intra-SM CUDA Core-Tensor Core Parallelism. We therefore propose Plasticine to exploit the intraSM parallelism for maximizing the GPU throughput. Plasticine involves compilation and runtime schedule to achieve the above purpose. Experimental results on an Nvidia 2080Ti GPU show that Plasticine improves the system-wide throughput by 15.3% compared with prior co-location work.","tags":[],"title":"Exploiting Intra-SM Parallelism in GPUs via Persistent and Elastic Blocks","type":"publication"},{"authors":["Weihao Cui","Quan Chen","Han Zhao","Mengze Wei","Xiaoxin Tang","Minyi Guo"],"categories":null,"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"87f49d02b51ca35b95de223c0f2c271d","permalink":"https://example.com/publication/2020ebird2/","publishdate":"2022-03-01T00:00:00Z","relpermalink":"/publication/2020ebird2/","section":"publication","summary":"We aim to tackle existing problems about deep learning serving on GPUs in the view of the system. GPUs have been widely adopted to serve online deep learning-based services that have stringent QoS(Quality-of-Service) requirements. However, emerging deep learning serving systems often result in poor responsiveness and low throughput of the inferences that damage user experience and increase the number of GPUs required to host an online service. Our investigation shows that the poor batching operation and the lack of data transfer-computation overlap are the root causes of the poor responsiveness and low throughput. To this end, we propose E2bird, a deep learning serving system that is comprised of a GPU-resident memory pool, a multi-granularity inference engine, and an elastic batch scheduler. The memory pool eliminates the unnecessary waiting of the batching operation and enables data transfer-computation overlap. The inference engine enables concurrent execution of different batches, improving the GPU resource utilization. The batch scheduler organizes inferences elastically to guarantee the QoS. Our experimental results on an Nvidia Titan RTX GPU show that E2bird reduces the response latency of inferences by up to 82.4% and improves the throughput by up to 62.8% while guaranteeing the QoS target compared with TensorFlow Serving.","tags":[],"title":"E2bird: Enhanced Elastic Batch for Improving Responsiveness and Throughput of Deep Learning Services","type":"publication"},{"authors":["Han Zhao","Weihao Cui","Quan Chen","Jingwen Leng","Kai Yu","Deze Zeng","Chao Li","Minyi Guo"],"categories":null,"content":"","date":1582848000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582848000,"objectID":"6dcb6755c5a92512a84edff78ea08ef0","permalink":"https://example.com/publication/2020coda/","publishdate":"2020-02-28T00:00:00Z","relpermalink":"/publication/2020coda/","section":"publication","summary":"While deep neural network (DNN) models are often trained on GPUs, many companies and research institutes build GPU clusters that are shared by different groups. On such GPU cluster, DNN training jobs also require CPU cores to run pre-processing, gradient synchronization. Our investigation shows that the number of cores allocated to a training job signiﬁcantly impact its performance. To this end, we characterize representative deep learning models on their requirement for CPU cores under different GPU resource conﬁgurations, and study the sensitivity of these models to other CPU-side shared resources. Based on the characterization, we propose CODA, a scheduling system that is comprised of an adaptive CPU allocator, a real-time contention eliminator, and a multi-array job scheduler. Experimental results show that CODA improves GPU utilization by 20.8% on average without increasing the queuing time of CPU jobs.","tags":[],"title":"CODA: Improving Resource Utilization by Slimming and Co-locating DNN and CPU Jobs","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://example.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Han Zhao","Quan Chen","Yuxian Qiu","Ming Wu","Yao Shen","Jingwen Leng","Chao Li","Minyi Guo"],"categories":null,"content":"","date":1546473600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546473600,"objectID":"d555f91c7b26633b5093ca2db749c9cf","permalink":"https://example.com/publication/2019bats/","publishdate":"2019-01-03T00:00:00Z","relpermalink":"/publication/2019bats/","section":"publication","summary":"Emerging GPUs have multiple Streaming Multiprocessors (SM), while each SM is comprised of CUDA Cores and Tensor Cores. While CUDA Cores do the general computation, Tensor Cores are designed to speed up matrix multiplication for deep learning applications. However, a GPU kernel often either uses CUDA Cores or Tensor Cores, leaving the other processing units idle. Although many prior research works have been proposed to co-locate kernels to improve GPU utilization, they cannot leverage the Intra-SM CUDA Core-Tensor Core Parallelism. Speciﬁcally, ISPA designs persistent and elastic block to solve the thread slot and shared memory contention between co-located kernels. ISPA also adopts the register allocation method to manage the register contention. These resource management methods are applicable for both white-box kernels and cudnn kernels. Experimental results on an Nvidia 2080Ti GPU show that ISPA improves the system-wide throughput by 15.3% for white-box workloads, and 7.1% for cudnn-based workloads compared with prior co-location work.","tags":[],"title":"Bandwidth and Locality Aware Task-stealing for Manycore Architectures with Bandwidth-Asymmetric Memory","type":"publication"}]