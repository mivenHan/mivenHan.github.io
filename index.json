
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I am an assistant researcher at Computer Science and Engineering Department in Shanghai Jiao Tong University. I received the Master and Ph.D. degrees from Shanghai Jiao Tong University under the supervision of Prof. Quan Chen and Prof. Minyi Guo. My past research focused on High performance computing on CPU architecture, Task scheduling on GPU architecture, Resource management in Datacenter, and DNN inference system design.\nWhile I will continue to dive into the above four areas for further research, my future research will cover more cloud computing topics such as GPU serverless and GPU virtualization. Specically, my future research could be divided into four aspects: Fine-grained GPU resource division, Multi-worker scheduling for DNN tasks, GPU serverless and Resource management in Datacenter.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am an assistant researcher at Computer Science and Engineering Department in Shanghai Jiao Tong University. I received the Master and Ph.D. degrees from Shanghai Jiao Tong University under the supervision of Prof.","tags":null,"title":"Han Zhao 赵涵","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://example.com/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Han Zhao","Weihao Cui","Quan Chen","Minyi Guo"],"categories":null,"content":"","date":1664755200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664755200,"objectID":"2cd5788e96f147b85fa08d85895dc6c2","permalink":"https://example.com/publication/2022ispa/","publishdate":"2022-10-03T00:00:00Z","relpermalink":"/publication/2022ispa/","section":"publication","summary":"Emerging GPUs have multiple Streaming Multiprocessors (SM), while each SM is comprised of CUDA Cores and Tensor Cores. While CUDA Cores do the general computation, Tensor Cores are designed to speed up matrix multiplication for deep learning applications. However, a GPU kernel often either uses CUDA Cores or Tensor Cores, leaving the other processing units idle. Although many prior research works have been proposed to co-locate kernels to improve GPU utilization, they cannot leverage the Intra-SM CUDA Core-Tensor Core Parallelism. Speciﬁcally, ISPA designs persistent and elastic block to solve the thread slot and shared memory contention between co-located kernels. ISPA also adopts the register allocation method to manage the register contention. These resource management methods are applicable for both white-box kernels and cudnn kernels. Experimental results on an Nvidia 2080Ti GPU show that ISPA improves the system-wide throughput by 15.3% for white-box workloads, and 7.1% for cudnn-based workloads compared with prior co-location work.","tags":[],"title":"ISPA: Exploiting Intra-SM Parallelism in GPUs via Fine-grained Resource Management","type":"publication"},{"authors":null,"categories":null,"content":"Overview This is a National key research program of China. In this work, I was responsible for building integrated big-data platform. We tried to implement a platform which can schedule between different systems and different user applications. I developed a common interface for collecting runtime information for different system and did some research about scheduling between applications.\n","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664582400,"objectID":"f5eb4859f936cdd9364c645b1d4ac08c","permalink":"https://example.com/post/2019-973/","publishdate":"2022-10-01T00:00:00Z","relpermalink":"/post/2019-973/","section":"post","summary":"973 Program, Main Student Member, 2016-2019","tags":null,"title":"Urban Big Data Collaborative Computing System","type":"post"},{"authors":null,"categories":null,"content":"Overview 在研项目。\n","date":1661990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661990400,"objectID":"52513300be97e53445eb7db61785c12b","permalink":"https://example.com/post/2022-%E5%9B%BD%E9%98%B2%E4%BA%91/","publishdate":"2022-09-01T00:00:00Z","relpermalink":"/post/2022-%E5%9B%BD%E9%98%B2%E4%BA%91/","section":"post","summary":"华为委托研究项目, Project Manager, 2022-2023","tags":null,"title":"单节点到多节点并行应用源到源翻译工具项目","type":"post"},{"authors":null,"categories":null,"content":"Overview 在研项目。\n","date":1661990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661990400,"objectID":"6b32b5ebf525d3734518eaa90a4d5663","permalink":"https://example.com/post/2022-huawei-2/","publishdate":"2022-09-01T00:00:00Z","relpermalink":"/post/2022-huawei-2/","section":"post","summary":"华为委托研究项目, Project Manager, 2022-2023","tags":null,"title":"源码并行化检测与提示工具项目","type":"post"},{"authors":null,"categories":null,"content":"Overview 国防科技创新特区项目。\n","date":1659312000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659312000,"objectID":"8da2bfa1daa1d001b90c2199925095b5","permalink":"https://example.com/post/2022-huawei-1/","publishdate":"2022-08-01T00:00:00Z","relpermalink":"/post/2022-huawei-1/","section":"post","summary":"国防科技创新特区项目, Main Student Member, 2020-2022","tags":null,"title":"云端协同海洋目标识别与分析","type":"post"},{"authors":["Weihao Cui","Han Zhao","Quan Chen","Hao Wei","Zirui Li","Deze Zeng","Chao Li","Minyi Guo"],"categories":null,"content":"","date":1649721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649721600,"objectID":"3b514d64d1f04bef08c99c97cbe18c03","permalink":"https://example.com/publication/2022dvabatch/","publishdate":"2022-04-12T00:00:00Z","relpermalink":"/publication/2022dvabatch/","section":"publication","summary":"The DNN inferences are often batched for better utilizing the hardware in existing DNN serving systems. However, DNN serving exhibits diversity in many aspects, such as input, operator, and load. The unawareness of these diversities results in inefficient processing. Our investigation shows that the inefficiency roots in the feature of the existing batching mechanism- one entry and one exit. Therefore, we propose DVABatch, a runtime batching system that enables the multi-entry multiexit batching scheme. We first abstract three meta operations, new, stretch, and split, for adjusting the ongoing batch of queries to achieve the multi-entry multi-exit scheme. The meta operations could be used to form different scheduling logics for different diversities. To deliver the meta operations to an ongoing batch, we slice the DNN models into multiple stages. Each stage corresponds to one executor, which is managed by a state transition diagram. Compared with state-of-the-art solutions, our experimental results show that DVABatch reduces 46.4% average latency and achieves up to 2.12× throughput improvement.","tags":[],"title":"DVABatch: Diversity-aware Multi-Entry Multi-Exit Batching for Efficient Processing of DNN Services on GPUs","type":"publication"},{"authors":["Han Zhao","Weihao Cui","Quan Chen","Youtao Zhang","Yanchao Lu","Chao Li","Jingwen Leng","Minyi Guo"],"categories":null,"content":"","date":1649376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649376000,"objectID":"8faef015b3092521153503c51ca6410c","permalink":"https://example.com/publication/2022tacker/","publishdate":"2022-04-08T00:00:00Z","relpermalink":"/publication/2022tacker/","section":"publication","summary":"The proliferation of machine learning applications has promoted both CUDA Cores and Tensor Cores’ integration to meet their acceleration demands. While studies have shown that co-locating multiple tasks on the same GPU can effectively improve system throughput and resource utilization, existing schemes focus on scheduling the resources of traditional CUDA Cores and thus lack the ability to exploit the parallelism between Tensor Cores and CUDA Cores. In this paper, we propose Tacker, a static kernel fusion and scheduling approach to improve GPU utilization of both types of cores while ensuring the QoS (Quality-of-Service) of co-located tasks. Tacker consists of a Tensor-CUDA Core kernel fuser, a duration predictor for fused kernels, and a runtime QoS-aware kernel manager. The kernel fuser enables the ﬂexible fusion of kernels that use Tensor Cores and CUDA Cores, respectively. The duration predictor precisely predicts the duration of the fused kernels. Finally, the kernel manager invokes the fused kernel or the original kernel based on the QoS headroom of latency-critical tasks to improve the system throughput. Our experimental results show that Tacker improves the throughput of best-effort applications compared with state-of-the-art solutions by 18.6% on average, while ensuring the QoS of latency-critical tasks.","tags":[],"title":"Tacker:Tensor-CUDA Core Kernel Fusion for Improving the GPU Utilization while Ensuring QoS","type":"publication"},{"authors":["Weihao Cui","Han Zhao","Quan Chen","Ningxin Zheng","Jingwen Leng","Jieru Zhao","Zhuo Song","Tao Ma","Yong Yang","Chao Li","Minyi Guo"],"categories":null,"content":"","date":1635638400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635638400,"objectID":"ae77344fd7ff0c1560b0cdba81e9a2c8","permalink":"https://example.com/publication/2021abacus/","publishdate":"2021-10-31T00:00:00Z","relpermalink":"/publication/2021abacus/","section":"publication","summary":"While user-facing services experience diurnal load patterns, colocating services improve hardware utilization. Prior work on colocating services on GPUs run queries sequentially, as the latencies of the queries are neither stable nor predictable when running simultaneously. The input sensitiveness and the non-deterministic operator overlap are two primary factors of the latency unpredictability. Hence, We propose Abacus, a runtime system that runs multiple services simultaneously. Abacus enables deterministic operator overlap to enforce latency predictability. Abacus composes of an overlap-aware latency predictor, a headroom-based query controller, and segmental model executors. The predictor predicts the latencies of the deterministic operator overlap. The controller determines the appropriate operator overlap for the QoS guarantee of all the services. The executors run the operators as needed to support the deterministic operator overlap. Our evaluation shows that Abacus reduces 51.3% of the QoS violation and improves the throughput by 29.8% on average compared with state-of-the-art solutions.","tags":[],"title":"Enable Simultaneous DNN Services Based on Deterministic Operator Overlap and Precise Latency Prediction","type":"publication"},{"authors":null,"categories":null,"content":"Overview 在人机物融合的环境下，应用需要使用分布在整个互联网、移动互联网和物联网之上的任意数据中心、终端甚至网络上的各种软硬件及数据和服务资源，这些资源呈现出与传统云计算资源不同的诸多新特性：资源访问更加泛在多元，包括多种不同的网络协议，导致资源的接入适配难以统一；资源来源更加广泛多样，不仅来自传统互联网的服务提供商，包括政府部门、通信运营商、传统制造商、乃至普通用户都可以贡献资源，导致资源的可信协同难以保证；资源使用更加随需多变，用户需求不再相对固定，而是要求在不同地点、不同时间、不同场景下即时按需地使用资源，导致资源的服务质量难以保障。传统云计算通常基于资源具备标准接入能力、来源可信、质量可控的假设，在横向上将越来越多的软硬件及数据和服务资源锁定在数据中心内部进行统一管理，无法满足人机物融合环境下分散在云、网、端的异构异质异态资源的管理要求。针对此问题，本课题将自底向上，从资源接入、资源发现、资源管理三个层面入手，开展如下三个方面的研究。\n泛在接入设备识别和跨协议通信技术。针对人机物融合环境下用户、异构终端和物理环境之间连接和交互的泛在性和多元性，研究基于信号时序特征的异构终端识别技术、基于协议特征分析的合作式设备共存和接入技术、基于信道状态和通用物理信号的跨协议通信和感知技术。 基于激励机制的云网端资源动态发现。针对人机物融合环境下资源供给和需求的广泛性和多样性，研究面向资源共享和交易的激励机制设计、基于分布式账本的资源可信发现方法，形成效益驱动的资源高效可信共享平台。 场景驱动的云网端资源多级协同管理。针对人机物融合环境下资源使用的随需性和多变性，研究场景驱动的异构资源敏感的自动任务调度技术、共享资源的细粒度管理技术、分布式多级资源的按需自适应管理技术，在满足服务质量要求的前提下提高资源利用率。 ","date":1630454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630454400,"objectID":"6dd42c02a6696200ea7becdf64740f96","permalink":"https://example.com/post/2021-%E4%BA%BA%E6%9C%BA%E7%89%A9/","publishdate":"2021-09-01T00:00:00Z","relpermalink":"/post/2021-%E4%BA%BA%E6%9C%BA%E7%89%A9/","section":"post","summary":"重点研发计划, Main Student Member, 2018-2021","tags":null,"title":"人机物融合的云计算架构与平台","type":"post"},{"authors":["Han Zhao","Weihao Cui","Quan Chen","Jieru Zhao","Jingwen Leng","Minyi Guo"],"categories":null,"content":"","date":1629244800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629244800,"objectID":"70099e905ed2c47e1281d3e73a821a79","permalink":"https://example.com/publication/2021plasticine/","publishdate":"2021-08-18T00:00:00Z","relpermalink":"/publication/2021plasticine/","section":"publication","summary":"Emerging GPUs have multiple Streaming Multiprocessors (SM), while each SM is comprised of CUDA Cores and Tensor Cores. While CUDA Cores do the general computation, Tensor Cores are designed to speed up matrix multiplication for deep learning applications. However, a GPU kernel often either uses CUDA Cores or Tensor Cores, leaving the other processing units idle. Although many prior research works have been proposed to co-locate kernels to improve GPU utilization, they cannot leverage the Intra-SM CUDA Core-Tensor Core Parallelism. We therefore propose Plasticine to exploit the intraSM parallelism for maximizing the GPU throughput. Plasticine involves compilation and runtime schedule to achieve the above purpose. Experimental results on an Nvidia 2080Ti GPU show that Plasticine improves the system-wide throughput by 15.3% compared with prior co-location work.","tags":[],"title":"Exploiting Intra-SM Parallelism in GPUs via Persistent and Elastic Blocks","type":"publication"},{"authors":null,"categories":null,"content":"Overview This is a joint research project between SJTU and HUAWEI. In this work, I was responsible for the scheme design and the Linux kernel programming. Overall, we identify the memory access of graph-related applications and improve their performance.\n","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"0cefe0657295bd5182b48b169290c1f4","permalink":"https://example.com/post/2019-huawei/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/post/2019-huawei/","section":"post","summary":"Huawei Innovative Research Program, Main Student Member, 2019-2020","tags":null,"title":"Memory Access Hotness Collection in the Linux","type":"post"},{"authors":["Weihao Cui","Quan Chen","Han Zhao","Mengze Wei","Xiaoxin Tang","Minyi Guo"],"categories":null,"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"87f49d02b51ca35b95de223c0f2c271d","permalink":"https://example.com/publication/2020ebird2/","publishdate":"2022-03-01T00:00:00Z","relpermalink":"/publication/2020ebird2/","section":"publication","summary":"We aim to tackle existing problems about deep learning serving on GPUs in the view of the system. GPUs have been widely adopted to serve online deep learning-based services that have stringent QoS(Quality-of-Service) requirements. However, emerging deep learning serving systems often result in poor responsiveness and low throughput of the inferences that damage user experience and increase the number of GPUs required to host an online service. Our investigation shows that the poor batching operation and the lack of data transfer-computation overlap are the root causes of the poor responsiveness and low throughput. To this end, we propose E2bird, a deep learning serving system that is comprised of a GPU-resident memory pool, a multi-granularity inference engine, and an elastic batch scheduler. The memory pool eliminates the unnecessary waiting of the batching operation and enables data transfer-computation overlap. The inference engine enables concurrent execution of different batches, improving the GPU resource utilization. The batch scheduler organizes inferences elastically to guarantee the QoS. Our experimental results on an Nvidia Titan RTX GPU show that E2bird reduces the response latency of inferences by up to 82.4% and improves the throughput by up to 62.8% while guaranteeing the QoS target compared with TensorFlow Serving.","tags":[],"title":"E2bird: Enhanced Elastic Batch for Improving Responsiveness and Throughput of Deep Learning Services","type":"publication"},{"authors":["Han Zhao","Weihao Cui","Quan Chen","Jingwen Leng","Kai Yu","Deze Zeng","Chao Li","Minyi Guo"],"categories":null,"content":"","date":1582848000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582848000,"objectID":"6dcb6755c5a92512a84edff78ea08ef0","permalink":"https://example.com/publication/2020coda/","publishdate":"2020-02-28T00:00:00Z","relpermalink":"/publication/2020coda/","section":"publication","summary":"While deep neural network (DNN) models are often trained on GPUs, many companies and research institutes build GPU clusters that are shared by different groups. On such GPU cluster, DNN training jobs also require CPU cores to run pre-processing, gradient synchronization. Our investigation shows that the number of cores allocated to a training job signiﬁcantly impact its performance. To this end, we characterize representative deep learning models on their requirement for CPU cores under different GPU resource conﬁgurations, and study the sensitivity of these models to other CPU-side shared resources. Based on the characterization, we propose CODA, a scheduling system that is comprised of an adaptive CPU allocator, a real-time contention eliminator, and a multi-array job scheduler. Experimental results show that CODA improves GPU utilization by 20.8% on average without increasing the queuing time of CPU jobs.","tags":[],"title":"CODA: Improving Resource Utilization by Slimming and Co-locating DNN and CPU Jobs","type":"publication"},{"authors":null,"categories":null,"content":"Overview This is a joint research project between SJTU and ALIBABA. In this work, I was responsible for the data collection for general Cudnn APIs and modeling between the QoS and resource usage. Overall, we guarantee the QoS of latency-sensitive applications and improve the GPU utilization.\n","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"86abb9e50bb8a654fef21da061270589","permalink":"https://example.com/post/2018-alibaba/","publishdate":"2019-09-01T00:00:00Z","relpermalink":"/post/2018-alibaba/","section":"post","summary":"Alibaba Innovative Research Program, Main Student Member, 2018-2019","tags":null,"title":"GPU Resource Sharing based on QoS-awareness","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://example.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Han Zhao","Quan Chen","Yuxian Qiu","Ming Wu","Yao Shen","Jingwen Leng","Chao Li","Minyi Guo"],"categories":null,"content":"","date":1546473600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546473600,"objectID":"d555f91c7b26633b5093ca2db749c9cf","permalink":"https://example.com/publication/2019bats/","publishdate":"2019-01-03T00:00:00Z","relpermalink":"/publication/2019bats/","section":"publication","summary":"Emerging GPUs have multiple Streaming Multiprocessors (SM), while each SM is comprised of CUDA Cores and Tensor Cores. While CUDA Cores do the general computation, Tensor Cores are designed to speed up matrix multiplication for deep learning applications. However, a GPU kernel often either uses CUDA Cores or Tensor Cores, leaving the other processing units idle. Although many prior research works have been proposed to co-locate kernels to improve GPU utilization, they cannot leverage the Intra-SM CUDA Core-Tensor Core Parallelism. Speciﬁcally, ISPA designs persistent and elastic block to solve the thread slot and shared memory contention between co-located kernels. ISPA also adopts the register allocation method to manage the register contention. These resource management methods are applicable for both white-box kernels and cudnn kernels. Experimental results on an Nvidia 2080Ti GPU show that ISPA improves the system-wide throughput by 15.3% for white-box workloads, and 7.1% for cudnn-based workloads compared with prior co-location work.","tags":[],"title":"Bandwidth and Locality Aware Task-stealing for Manycore Architectures with Bandwidth-Asymmetric Memory","type":"publication"},{"authors":null,"categories":null,"content":"Overview This is a joint research project between SJTU and Huawei (Canada). In this work, I was responsible for the system design and analysis. We modiﬁed the communication mechanism between Namenode and Datanode and adopted time slice based method to control the IO share of each user.\n","date":1535760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535760000,"objectID":"cac480e81dfcb400a5fa4379c2efc488","permalink":"https://example.com/post/2017-huawei/","publishdate":"2018-09-01T00:00:00Z","relpermalink":"/post/2017-huawei/","section":"post","summary":"Huawei Innovative Research Program, Main Student Member, 2017-2018","tags":null,"title":"Large-scale heterogeneous processing platform","type":"post"},{"authors":null,"categories":null,"content":"Overview This is a joint research preject between SJTU and Huawei (Shanghai). In this work, I was responsible for the security enforcement module. I proposed four schemes to adjust the veriﬁcation between master node and slave nodes for better security without sacriﬁcing performance signiﬁcantly.\n","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"0c03fbf6178436c0fee24f4fe196c1fd","permalink":"https://example.com/post/2016-huawei/","publishdate":"2017-09-01T00:00:00Z","relpermalink":"/post/2016-huawei/","section":"post","summary":"Huawei Innovative Research Program, Main Student Member, 2016-2017","tags":null,"title":"Redis Enforcement","type":"post"}]